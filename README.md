<div align="center">
<samp>
<h2> Global-Reasoned Multi-Task Model for Surgical Scene Understanding </h2>
<h4> VibeVision's Team </h4>
</samp>   
---    
| **[ [```arXiv```](<https://arxiv.org/abs/2201.11957>) ]** |**[ [```Paper```](<https://ieeexplore.ieee.org/document/9695281>) ]** |**[ [```YouTube```](<https://youtu.be/UOIcp3y4o1U>) ]** |
|:-------------------:|:-------------------:|:-------------------:|
ICRA 2022, IEEE Robotics and Automation Letters (RA-L)
</div>
If you find our code or paper useful, please use the following citation:
```bibtex
@article{seenivasan2022global,
  title={Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding},
  author={VibeVision's Team},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE}
}
```
---
## Introduction
This globally-reasoned multi-task model enables scene understanding and performs instrument segmentation and tool-tissue interaction detection. Using a sequential optimization technique, the proposed multi-task model outperforms other state-of-the-art single-task models on the MICCAI endoscopic vision challenge 2018 dataset.
## How to Use
 First, install the prerequisites mentioned. Then, follow the steps for dataset usage and model training and evaluation. Our setup section provides environment files for easy installation using conda.
---
## Acknowledgement
Several parts of our code are adopted and modified from :
1. Visual-Semantic Graph Attention Network for Human-Object Interaction Detecion
2. Graph-Based Global Reasoning Networks
---
## Contact
For any queries, please contact VibeVision's Team through their GitHub profiles.
